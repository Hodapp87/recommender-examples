{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (Movie Reviews)\n",
    "- Author: Chris Hodapp\n",
    "- Date: 2018-02-04\n",
    "\n",
    "This was done as part of a [SharpestMinds](https://www.sharpestminds.com/) skills test that was left fairly open-ended, but for which the goal was to build a recommender system for movie reviews.  It's based around the [movielens 100k](https://grouplens.org/datasets/movielens/100k/) dataset, and the code below assumes that `ml-100k` from there has been downloaded and uncompressed in the local directory.\n",
    "\n",
    "In addition to the normal steps of loading data and doing basic transformations, this works through implementations of:\n",
    "\n",
    "- Slope One Predictors - a collaborative filtering method, particularly, a neighborhood model.\n",
    "- An \"SVD\" algorithm which [Simon Funk](http://sifter.org/~simon/journal/20061211.html) popularized for the Netflix prize - another collaborative filtering method, this one a latent factor model based on matrix factorization.\n",
    "\n",
    "It also reproduces similar results with [scikit-surprise](http://surpriselib.com/), which implements these algorithms (and many others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- Bipolar Slope One?\n",
    "- Read Netflix post\n",
    "- Content-based filtering?\n",
    "- Maybe implement something easy like NMF?\n",
    "- Finish blog post and link to it\n",
    "- Visualization of latent factors\n",
    "- Explain code a bit better (ditch the formal Python version)\n",
    "- Put utility matrix someplace else since I only need it for Slope One\n",
    "- How far separate should this be from the Slope One blog post?\n",
    "  - Why don't I just export it to Markdown, merge in all the Slope One post content, and put this file on my blag as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", header=None,\n",
    "                 names=(\"user_id\", \"movie_id\", \"rating\", \"time\"))\n",
    "# Convert Unix seconds to a Pandas timestamp:\n",
    "ml[\"time\"] = pd.to_datetime(ml[\"time\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-01-07 14:20:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-12-03 17:51:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-04-03 18:34:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-02-01 09:20:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-31 21:16:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating                time\n",
       "0      196       242       3 1997-12-04 15:55:49\n",
       "1      186       302       3 1998-04-04 19:22:22\n",
       "2       22       377       1 1997-11-07 07:18:36\n",
       "3      244        51       2 1997-11-27 05:02:03\n",
       "4      166       346       1 1998-02-02 05:33:16\n",
       "5      298       474       4 1998-01-07 14:20:06\n",
       "6      115       265       2 1997-12-03 17:51:28\n",
       "7      253       465       5 1998-04-03 18:34:27\n",
       "8      305       451       3 1998-02-01 09:20:17\n",
       "9        6        86       3 1997-12-31 21:16:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 1683, 1588752)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_user  = int(ml[\"user_id\"].max() + 1)\n",
    "max_movie = int(ml[\"movie_id\"].max() + 1)\n",
    "max_user, max_movie, max_user * max_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of data sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06294248567429025"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape[0] / (max_user * max_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We need an average rating value for some models, but it might make more sense to have that average be weighted by the movie's popularity in some fashion.  Also, we read in the list of movie names below in order to get some more comprehensible information out of this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\n",
    "    \"ml-100k/u.item\", sep=\"|\", header=None,\n",
    "    encoding = \"ISO-8859-1\", index_col=0,\n",
    "    names=(\"movie_id\", \"movie_title\"), usecols=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_group = ml.groupby(\"movie_id\")\n",
    "movie_stats = names.\\\n",
    "    join(movie_group.size().rename(\"num_ratings\")).\\\n",
    "    join(movie_group.mean()[\"rating\"].rename(\"avg_rating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by number of ratings and looking at the movie titles, this looks pretty sensible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Star Wars (1977)</td>\n",
       "      <td>583</td>\n",
       "      <td>4.358491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Contact (1997)</td>\n",
       "      <td>509</td>\n",
       "      <td>3.803536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Fargo (1996)</td>\n",
       "      <td>508</td>\n",
       "      <td>4.155512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Return of the Jedi (1983)</td>\n",
       "      <td>507</td>\n",
       "      <td>4.007890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Liar Liar (1997)</td>\n",
       "      <td>485</td>\n",
       "      <td>3.156701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>English Patient, The (1996)</td>\n",
       "      <td>481</td>\n",
       "      <td>3.656965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Scream (1996)</td>\n",
       "      <td>478</td>\n",
       "      <td>3.441423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>452</td>\n",
       "      <td>3.878319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Air Force One (1997)</td>\n",
       "      <td>431</td>\n",
       "      <td>3.631090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Independence Day (ID4) (1996)</td>\n",
       "      <td>429</td>\n",
       "      <td>3.438228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Raiders of the Lost Ark (1981)</td>\n",
       "      <td>420</td>\n",
       "      <td>4.252381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Godfather, The (1972)</td>\n",
       "      <td>413</td>\n",
       "      <td>4.283293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "      <td>394</td>\n",
       "      <td>4.060914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Twelve Monkeys (1995)</td>\n",
       "      <td>392</td>\n",
       "      <td>3.798469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Silence of the Lambs, The (1991)</td>\n",
       "      <td>390</td>\n",
       "      <td>4.289744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Jerry Maguire (1996)</td>\n",
       "      <td>384</td>\n",
       "      <td>3.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Rock, The (1996)</td>\n",
       "      <td>378</td>\n",
       "      <td>3.693122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Empire Strikes Back, The (1980)</td>\n",
       "      <td>367</td>\n",
       "      <td>4.204360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Star Trek: First Contact (1996)</td>\n",
       "      <td>365</td>\n",
       "      <td>3.660274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Titanic (1997)</td>\n",
       "      <td>350</td>\n",
       "      <td>4.245714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               movie_title  num_ratings  avg_rating\n",
       "movie_id                                                           \n",
       "50                        Star Wars (1977)          583    4.358491\n",
       "258                         Contact (1997)          509    3.803536\n",
       "100                           Fargo (1996)          508    4.155512\n",
       "181              Return of the Jedi (1983)          507    4.007890\n",
       "294                       Liar Liar (1997)          485    3.156701\n",
       "286            English Patient, The (1996)          481    3.656965\n",
       "288                          Scream (1996)          478    3.441423\n",
       "1                         Toy Story (1995)          452    3.878319\n",
       "300                   Air Force One (1997)          431    3.631090\n",
       "121          Independence Day (ID4) (1996)          429    3.438228\n",
       "174         Raiders of the Lost Ark (1981)          420    4.252381\n",
       "127                  Godfather, The (1972)          413    4.283293\n",
       "56                     Pulp Fiction (1994)          394    4.060914\n",
       "7                    Twelve Monkeys (1995)          392    3.798469\n",
       "98        Silence of the Lambs, The (1991)          390    4.289744\n",
       "237                   Jerry Maguire (1996)          384    3.710938\n",
       "117                       Rock, The (1996)          378    3.693122\n",
       "172        Empire Strikes Back, The (1980)          367    4.204360\n",
       "222        Star Trek: First Contact (1996)          365    3.660274\n",
       "313                         Titanic (1997)          350    4.245714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_stats.sort_values(\"num_ratings\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_train, ml_test = sklearn.model_selection.train_test_split(ml, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to utility matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a mask for some later steps, hence the m > 0 step; ratings go only from 1 to 5, so values of 0 are automatically unknown/missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df2mat(df):\n",
    "    m = np.zeros((max_user, max_movie))\n",
    "    m[df[\"user_id\"], df[\"movie_id\"]] = df[\"rating\"]\n",
    "    return m, m > 0\n",
    "ml_mat_train, ml_mask_train = df2mat(ml_train)\n",
    "ml_mat_test,  ml_mask_test  = df2mat(ml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were an actual large amount of data, which a 944x1683 matrix doesn't really count as, you'd probably want [sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html) and to use 8-bit ints rather than 32-bit floats, for instance:\n",
    "\n",
    "```python\n",
    "ml_mat = scipy.sparse.coo_matrix(\n",
    "    (ml[\"rating\"], (ml[\"user_id\"], ml[\"movie_id\"])),\n",
    "    shape=(max_user, max_movie),\n",
    "    dtype=np.int8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope One implementation\n",
    "\n",
    "- Based on:  [Slope One Predictors for Online Rating-Based Collaborative Filtering](https://arxiv.org/pdf/cs/0702144v1.pdf)\n",
    "- TODO: This needs better explanation but I'm not sure if it should reside here, in the Python code, or in the blag post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deviation(M, mask):\n",
    "    m,n = M.shape\n",
    "    m2 = mask.astype(np.int)\n",
    "    counts = m2.T @ m2\n",
    "    S = m2.T @ M\n",
    "    diffs = S.T - S\n",
    "    dev = diffs / np.maximum(1, counts)\n",
    "    return dev, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of 'deviation' above might be less-optimal vastly larger matrices. For one thing, Slope One doesn't really *need* a utility matrix, though it's easier from one. One could readily compute deviation from the list of ratings, though I don't know a fast way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one(M, mask, dev, counts, u, j, weighted = False):\n",
    "    m,n = M.shape\n",
    "    # S_u is a mask over M's columns for items user 'u' rated:\n",
    "    S_u = mask[u, :]\n",
    "    if weighted:\n",
    "        # In 'Weighted Slope One', we sum over everything user 'u' rated,\n",
    "        # regardless of whether other users rated both this and item j:\n",
    "        S_u[j] = False\n",
    "        c_j = counts[j, S_u]\n",
    "        devs = dev[j, S_u]\n",
    "        u = M[u, S_u]\n",
    "        return ((devs + u) * c_j).sum() / max(1.0, c_j.sum())\n",
    "    else:\n",
    "        # In the 'Slope One' formula we are summing over R_j, which is:\n",
    "        # Every item 'i' (i != j), such that: user 'u' rated item 'i', and\n",
    "        # at least one other user rated both item 'i' and item 'j'.\n",
    "        # Below we compute this likewise as a mask over M's columns:\n",
    "        R_j = S_u * (counts[u, :] > 0)\n",
    "        R_j[j] = False\n",
    "        u = M[u, R_j].sum()\n",
    "        devs = dev[j, R_j].sum()\n",
    "        card = max(1.0, R_j.sum())\n",
    "        return (u + devs) / card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(M, mask, dev, counts, dataframe, weighted=False):\n",
    "    err_mae = 0\n",
    "    err_rms = 0\n",
    "    for row in dataframe.itertuples():\n",
    "        p = predict_one(M, mask, dev, counts,\n",
    "                        row.user_id, row.movie_id, weighted=weighted)\n",
    "        err_mae += np.abs(p - row.rating)\n",
    "        err_rms += np.square(p - row.rating)\n",
    "    err_mae = err_mae / len(dataframe)\n",
    "    err_rms = np.sqrt(err_rms / len(dataframe))\n",
    "    return err_mae, err_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute deviation (which is basically our model) from training:\n",
    "dev, counts = deviation(ml_mat_train, ml_mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.6686189218728723, RMS=0.8600008184758705\n",
      "Testing error: MAE=0.7625418649564116, RMS=0.9825453599122679\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train, True)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test,  True)\n",
    "# why must I pass both dataframe and matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.7386526192700555, RMS=0.9841829878336311\n",
      "Testing error: MAE=0.8917393081693394, RMS=1.2316649773953448\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"SVD\" based algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References on this can be found in a few places.  [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) in Surprise gives enough formulas to implement from. Simon Funk's post [Netflix Update: Try This at Home](http://sifter.org/~simon/journal/20061211.html) is an excellent overview of the rationale and of some practical concerns on how to run this on the much larger Netflix dataset (100,000,000 ratings, instead of 100,000). The paywalled article [Matrix Factorization Techniques for Recommender Systems](http://ieeexplore.ieee.org/abstract/document/5197422/) gives a little background from a higher level.\n",
    "\n",
    "It is also a fairly simple algorithm, but its naming (or lack thereof) is a little confusing. Despite being called an \"SVD\" algorithm, it doesn't exactly compute Singular Value Decomposition, but it's SVD-like. It is still more or less a [matrix completion](https://en.wikipedia.org/wiki/Matrix_completion) problem - but it is a bit different from many matrix completion methods that explicitly use SVD, like [altMinSense & altMinComplete](https://arxiv.org/pdf/1212.0467), which [matrix-completion-whirlwind](https://github.com/asberk/matrix-completion-whirlwind/blob/master/matrix_completion_master.ipynb) has an excellent explanation and implementation of.\n",
    "\n",
    "In short: This algorithm performs matrix completion via low-rank matrix factorization.  It does that factorization via [stochastic gradient-descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), and it does it without explicitly creating a utility matrix.\n",
    "\n",
    "(TODO: Explain utility matrix.)\n",
    "\n",
    "(TODO: Explain what matrices it's even building.)\n",
    "\n",
    "For completeness (and to verify it myself), I give the model and derive its gradients here.\n",
    "\n",
    "The prediction model is:\n",
    "\n",
    "$$\\hat{r}_{ui}=\\mu + b_i + b_u + q_i^\\top p_u$$\n",
    "\n",
    "where $u$ is a user, $i$ is an item, $\\mu$ is the overall average rating, $b_i$ and $b_u$ are per-item and per-user deviations respectively, and $q_i$ and $p_u$ are feature vectors.\n",
    "\n",
    "The goal is to minimize $L_2$-regularized squared error:\n",
    "\n",
    "$$E=\\sum_{r_{ui} \\in R_{\\textrm{train}}} \\left(r_{ui} - \\hat{r}_{ui}\\right)^2 + \\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right)$$\n",
    "\n",
    "This is easily differentiable with respect to model parameters $b_i$, $b_u$, $q_i$, and $p_u$, so a normal approach is gradient-descent.  Finding gradient with respect to $b_i$ is straightforward:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial b_i} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial b_i}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} 2\\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)(-1) + 2 \\lambda b_i \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + r_{ui} - \\hat{r}_{ui}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Gradient with respect to $p_u$ proceeds similarly:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial p_u} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial p_u}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)\\left(-\\frac{\\partial}{\\partial\n",
    "p_u}q_i^\\top p_u \\right) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)(-q_i^\\top) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - \\left(r_{ui} - \\hat{r}_{ui}\\right)q_i^\\top\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Gradient with respect to $b_u$ is identical form to $b_i$, and gradient with respect to $q_i$ is identical form to $p_u$, except that the variables switch places.  The full gradients then have the standard form for gradient descent, i.e. a summation of a gradient term for each individual data point, so they turn easily into update rules for each parameter (which match the ones in the Surprise link) after absorbing the leading 2 into learning rate $\\gamma$ and separating out the summation over each data point. That's given below, with $e_{ui}=r_{ui} - \\hat{r}_{ui}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_i' &= b_i - \\gamma\\frac{\\partial E}{\\partial b_i} &= b_i + \\gamma\\left(e_{ui} - \\lambda b_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_u} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_u + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_u' &= b_u - \\gamma\\frac{\\partial E}{\\partial b_u} &= b_u + \\gamma\\left(e_{ui} - \\lambda b_i \\right)\\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - e_{ui}q_i^\\top\\ \\ \\ &\\longrightarrow p_u' &= p_u - \\gamma\\frac{\\partial E}{\\partial p_u} &= p_u + \\gamma\\left(e_{ui}q_i - \\lambda p_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial q_i} &= 2 \\sum_{r_{ui}} \\lambda q_i - e_{ui}p_u^\\top\\ \\ \\ &\\longrightarrow q_i' &= q_i - \\gamma\\frac{\\partial E}{\\partial q_i} &= q_i + \\gamma\\left(e_{ui}p_u - \\lambda q_i \\right) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Below is a pretty direct implementation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (using Surprise defaults):\n",
    "gamma = 0.005\n",
    "lambda_ = 0.02\n",
    "factors = 100\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDModel(object):\n",
    "    def __init__(self, num_items, num_users, mean,\n",
    "                 num_factors = 100, init_variance = 0.1):\n",
    "        self.mu = mean\n",
    "        self.num_items = num_items\n",
    "        self.num_users = num_users\n",
    "        self.num_factors = num_factors\n",
    "        # Deviations, per-item:\n",
    "        self.b_i = np.zeros((num_items,))\n",
    "        # Deviations; per-user:\n",
    "        self.b_u = np.zeros((num_users,))\n",
    "        # Factor matrices:\n",
    "        self.q = np.random.randn(num_factors, num_items) * init_variance\n",
    "        self.p = np.random.randn(num_factors, num_users) * init_variance\n",
    "        # N.B. row I of q is item I's \"concepts\", so to speak;\n",
    "        # column U of p is how much user U belongs to each \"concept\"\n",
    "    \n",
    "    def predict(self, items, users):\n",
    "        \"\"\"Returns rating prediction for specific items and users.\n",
    "\n",
    "        Parameters:\n",
    "        items -- 1D array of item IDs\n",
    "        users -- 1D array of user IDs (same length as :items:)\n",
    "        \n",
    "        Returns:\n",
    "        ratings -- 1D array of predicted ratings (same length as :items:)\n",
    "        \"\"\"\n",
    "        # Note that we don't multiply p & q like matrices here,\n",
    "        # but rather, we just do row-by-row dot products.\n",
    "        # Matrix multiply would give us every combination of item and user,\n",
    "        # which isn't what we want.\n",
    "        return self.mu + \\\n",
    "               self.b_i[items] + \\\n",
    "               self.b_u[users] + \\\n",
    "               (self.q[:, items] * self.p[:, users]).sum(axis=0)\n",
    "    \n",
    "    def error(self, items, users, ratings):\n",
    "        \"\"\"Predicts over the given items and users, compares with the correct\n",
    "        ratings, and returns RMSE and MAE.\n",
    "        \n",
    "        Parameters:\n",
    "        items -- 1D array of item IDs\n",
    "        users -- 1D array of user IDs (same length as :items:)\n",
    "        ratings -- 1D array of 'correct' item ratings (same length as :items:)\n",
    "        \n",
    "        Returns:\n",
    "        rmse, mae -- Scalars for RMS error and mean absolute error\n",
    "        \"\"\"\n",
    "        p = self.predict(items, users)\n",
    "        d = p - ratings\n",
    "        rmse = np.sqrt(np.square(d).sum() / items.size)\n",
    "        mae = np.abs(d).sum() / items.size\n",
    "        return rmse, mae\n",
    "    \n",
    "    def update_by_gradient(self, i, u, r_ui, lambda_, gamma):\n",
    "        \"\"\"Perform a single gradient-descent update.\"\"\"\n",
    "        e_ui = r_ui - self.predict(i, u)\n",
    "        dbi = gamma * (e_ui - lambda_ * self.b_u[u])\n",
    "        dbu = gamma * (e_ui - lambda_ * self.b_i[i])\n",
    "        dpu = gamma * (e_ui * self.q[:,i] - lambda_ * self.p[:, u])\n",
    "        dqi = gamma * (e_ui * self.p[:,u] - lambda_ * self.q[:, i])\n",
    "        self.b_i[i] += dbi\n",
    "        self.b_u[u] += dbu\n",
    "        self.p[:,u] += dpu\n",
    "        self.q[:,i] += dqi\n",
    "    \n",
    "    def train(self, items, users, ratings, gamma = 0.005, lambda_ = 0.02,\n",
    "              num_epochs=20, epoch_callback=None):\n",
    "        \"\"\"Train with stochastic gradient-descent\"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            for idx in np.random.permutation(len(items)):\n",
    "                i, u, r_ui = items[idx], users[idx], ratings[idx]\n",
    "                self.update_by_gradient(i, u, r_ui, lambda_, gamma)\n",
    "            if epoch_callback: epoch_callback(self, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_train = ml_train[\"movie_id\"].values\n",
    "users_train = ml_train[\"user_id\"].values\n",
    "ratings_train = ml_train[\"rating\"].values\n",
    "movies_test = ml_test[\"movie_id\"].values\n",
    "users_test = ml_test[\"user_id\"].values\n",
    "ratings_test = ml_test[\"rating\"].values\n",
    "def at_epoch(self, epoch, num_epochs):\n",
    "    train_rmse, train_mae = self.error(movies_train, users_train, ratings_train)\n",
    "    test_rmse, test_mae = self.error(movies_test, users_test, ratings_test)\n",
    "    print(\"Epoch {:02d}/{}; Training: MAE={:.3f} RMSE={:.3f}, Testing: MAE={:.3f} RMSE={:.3f}\".\n",
    "          format(epoch + 1, num_epochs, train_mae, train_rmse, test_mae, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20; Training: MAE=0.805 RMSE=0.996, Testing: MAE=0.815 RMSE=1.005\n",
      "Epoch 02/20; Training: MAE=0.763 RMSE=0.955, Testing: MAE=0.783 RMSE=0.978\n",
      "Epoch 03/20; Training: MAE=0.741 RMSE=0.931, Testing: MAE=0.770 RMSE=0.966\n",
      "Epoch 04/20; Training: MAE=0.726 RMSE=0.914, Testing: MAE=0.763 RMSE=0.959\n",
      "Epoch 05/20; Training: MAE=0.714 RMSE=0.900, Testing: MAE=0.758 RMSE=0.954\n",
      "Epoch 06/20; Training: MAE=0.703 RMSE=0.887, Testing: MAE=0.755 RMSE=0.951\n",
      "Epoch 07/20; Training: MAE=0.694 RMSE=0.875, Testing: MAE=0.752 RMSE=0.949\n",
      "Epoch 08/20; Training: MAE=0.684 RMSE=0.864, Testing: MAE=0.751 RMSE=0.947\n",
      "Epoch 09/20; Training: MAE=0.675 RMSE=0.852, Testing: MAE=0.749 RMSE=0.945\n",
      "Epoch 10/20; Training: MAE=0.665 RMSE=0.840, Testing: MAE=0.747 RMSE=0.944\n",
      "Epoch 11/20; Training: MAE=0.655 RMSE=0.827, Testing: MAE=0.746 RMSE=0.942\n",
      "Epoch 12/20; Training: MAE=0.645 RMSE=0.814, Testing: MAE=0.745 RMSE=0.941\n",
      "Epoch 13/20; Training: MAE=0.634 RMSE=0.800, Testing: MAE=0.743 RMSE=0.940\n",
      "Epoch 14/20; Training: MAE=0.623 RMSE=0.786, Testing: MAE=0.742 RMSE=0.939\n",
      "Epoch 15/20; Training: MAE=0.611 RMSE=0.770, Testing: MAE=0.741 RMSE=0.938\n",
      "Epoch 16/20; Training: MAE=0.599 RMSE=0.755, Testing: MAE=0.740 RMSE=0.937\n",
      "Epoch 17/20; Training: MAE=0.586 RMSE=0.738, Testing: MAE=0.739 RMSE=0.937\n",
      "Epoch 18/20; Training: MAE=0.573 RMSE=0.722, Testing: MAE=0.738 RMSE=0.936\n",
      "Epoch 19/20; Training: MAE=0.559 RMSE=0.705, Testing: MAE=0.738 RMSE=0.935\n",
      "Epoch 20/20; Training: MAE=0.546 RMSE=0.688, Testing: MAE=0.737 RMSE=0.935\n"
     ]
    }
   ],
   "source": [
    "svd100 = SVDModel(max_movie, max_user, ml[\"rating\"].mean(), num_factors=100)\n",
    "svd100.train(movies_train, users_train, ratings_train, epoch_callback=at_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20; Training: MAE=0.811 RMSE=1.001, Testing: MAE=0.813 RMSE=1.001\n",
      "Epoch 02/20; Training: MAE=0.774 RMSE=0.969, Testing: MAE=0.780 RMSE=0.974\n",
      "Epoch 03/20; Training: MAE=0.758 RMSE=0.953, Testing: MAE=0.767 RMSE=0.962\n",
      "Epoch 04/20; Training: MAE=0.749 RMSE=0.944, Testing: MAE=0.759 RMSE=0.955\n",
      "Epoch 05/20; Training: MAE=0.743 RMSE=0.937, Testing: MAE=0.755 RMSE=0.951\n",
      "Epoch 06/20; Training: MAE=0.739 RMSE=0.933, Testing: MAE=0.751 RMSE=0.948\n",
      "Epoch 07/20; Training: MAE=0.736 RMSE=0.929, Testing: MAE=0.749 RMSE=0.945\n",
      "Epoch 08/20; Training: MAE=0.733 RMSE=0.926, Testing: MAE=0.747 RMSE=0.944\n",
      "Epoch 09/20; Training: MAE=0.731 RMSE=0.924, Testing: MAE=0.746 RMSE=0.943\n",
      "Epoch 10/20; Training: MAE=0.729 RMSE=0.922, Testing: MAE=0.745 RMSE=0.942\n",
      "Epoch 11/20; Training: MAE=0.728 RMSE=0.921, Testing: MAE=0.744 RMSE=0.941\n",
      "Epoch 12/20; Training: MAE=0.726 RMSE=0.919, Testing: MAE=0.743 RMSE=0.940\n",
      "Epoch 13/20; Training: MAE=0.726 RMSE=0.918, Testing: MAE=0.743 RMSE=0.940\n",
      "Epoch 14/20; Training: MAE=0.724 RMSE=0.917, Testing: MAE=0.742 RMSE=0.939\n",
      "Epoch 15/20; Training: MAE=0.724 RMSE=0.916, Testing: MAE=0.742 RMSE=0.939\n",
      "Epoch 16/20; Training: MAE=0.723 RMSE=0.915, Testing: MAE=0.741 RMSE=0.939\n",
      "Epoch 17/20; Training: MAE=0.722 RMSE=0.914, Testing: MAE=0.741 RMSE=0.938\n",
      "Epoch 18/20; Training: MAE=0.721 RMSE=0.913, Testing: MAE=0.741 RMSE=0.938\n",
      "Epoch 19/20; Training: MAE=0.720 RMSE=0.912, Testing: MAE=0.741 RMSE=0.938\n"
     ]
    }
   ],
   "source": [
    "# 2-dimensional factors, to make visualizing things easier:\n",
    "svd2 = SVDModel(max_movie, max_user, ml[\"rating\"].mean(), 2)\n",
    "svd2.train(ml_train[\"movie_id\"].values, ml_train[\"user_id\"].values, ml_train[\"rating\"].values, epoch_callback=at_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a latent-factor model, it might have some other things to tell us than predictions. Let's pick some common movies, and see how they map as \"concepts\".  We can easily pick common movies by number of ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = movie_stats.sort_values(\"num_ratings\", ascending=False)[:100]\n",
    "ids_top100 = top100.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = svd2.q[:,ids_top100].T\n",
    "means, stds = factors.mean(axis=0), factors.std(axis=0)\n",
    "factors[:] = (factors - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15))\n",
    "markers = [\"$ {} $\".format(\"\\ \".join(m.split(\" \")[:-1])) for m in top100[\"movie_title\"]]\n",
    "for i,item in enumerate(factors):\n",
    "    l = len(markers[i])\n",
    "    plt.scatter(item[0], item[1], marker = markers[i], alpha=0.75, s = 800 * l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations in `scikit-surprise`\n",
    "\n",
    "[Surprise](http://surpriselib.com/) contains implementations of many of the same things, so these are tested below. This same dataset is included as a built-in, but for consistency, we may as well load it from our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would I turn movies into factor scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ml[[\"user_id\", \"movie_id\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.model_selection.cross_validate(surprise.NormalPredictor(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SlopeOne(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SVD(), data, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

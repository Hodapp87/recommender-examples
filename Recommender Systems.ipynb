{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (Movie Reviews)\n",
    "- Author: Chris Hodapp\n",
    "- Date: 2018-02-04\n",
    "\n",
    "This was done as part of a [SharpestMinds](https://www.sharpestminds.com/) skills test that was left fairly open-ended, but for which the goal was to build a recommender system for movie reviews.  It's based around the [movielens 100k](https://grouplens.org/datasets/movielens/100k/) dataset, and the code below assumes that `ml-100k` from there has been downloaded and uncompressed in the local directory.\n",
    "\n",
    "In addition to the normal steps of loading data and doing basic transformations, this works through implementations of:\n",
    "\n",
    "- Slope One Predictors - a collaborative filtering method, particularly, a neighborhood model.\n",
    "- An \"SVD\" algorithm which [Simon Funk](http://sifter.org/~simon/journal/20061211.html) popularized for the Netflix prize - another collaborative filtering method, this one a latent factor model based on matrix factorization.\n",
    "\n",
    "It also reproduces similar results with [scikit-surprise](http://surpriselib.com/), which implements these algorithms (and many others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- Bipolar Slope One?\n",
    "- Read Netflix post\n",
    "- Content-based filtering?\n",
    "- Maybe implement something easy like NMF?\n",
    "- Finish blog post and link to it\n",
    "- Visualization of latent factors\n",
    "- Explain code a bit better (ditch the formal Python version)\n",
    "- Put utility matrix someplace else since I only need it for Slope One\n",
    "\n",
    "How far separate should this be from the Slope One blog post?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", header=None,\n",
    "                 names=(\"user_id\", \"movie_id\", \"rating\", \"time\"))\n",
    "# Convert Unix seconds to a Pandas timestamp:\n",
    "ml[\"time\"] = pd.to_datetime(ml[\"time\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-01-07 14:20:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-12-03 17:51:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-04-03 18:34:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-02-01 09:20:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-31 21:16:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating                time\n",
       "0      196       242       3 1997-12-04 15:55:49\n",
       "1      186       302       3 1998-04-04 19:22:22\n",
       "2       22       377       1 1997-11-07 07:18:36\n",
       "3      244        51       2 1997-11-27 05:02:03\n",
       "4      166       346       1 1998-02-02 05:33:16\n",
       "5      298       474       4 1998-01-07 14:20:06\n",
       "6      115       265       2 1997-12-03 17:51:28\n",
       "7      253       465       5 1998-04-03 18:34:27\n",
       "8      305       451       3 1998-02-01 09:20:17\n",
       "9        6        86       3 1997-12-31 21:16:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 1683, 1588752)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_user  = int(ml[\"user_id\"].max() + 1)\n",
    "max_movie = int(ml[\"movie_id\"].max() + 1)\n",
    "max_user, max_movie, max_user * max_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of data sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06294248567429025"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape[0] / (max_user * max_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We need an average rating value for some models, but it might make more sense to have that average be weighted by the movie's popularity in some fashion.  Also, we read in the list of movie names below in order to get some more comprehensible information out of this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\n",
    "    \"ml-100k/u.item\", sep=\"|\", header=None,\n",
    "    encoding = \"ISO-8859-1\", index_col=0,\n",
    "    names=(\"movie_id\", \"movie_title\"), usecols=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_group = ml.groupby(\"movie_id\")\n",
    "movie_stats = names.\\\n",
    "    join(movie_group.size().rename(\"num_ratings\")).\\\n",
    "    join(movie_group.mean()[\"rating\"].rename(\"avg_rating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by number of ratings and looking at the movie titles, this looks pretty sensible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Star Wars (1977)</td>\n",
       "      <td>583</td>\n",
       "      <td>4.358491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Contact (1997)</td>\n",
       "      <td>509</td>\n",
       "      <td>3.803536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Fargo (1996)</td>\n",
       "      <td>508</td>\n",
       "      <td>4.155512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Return of the Jedi (1983)</td>\n",
       "      <td>507</td>\n",
       "      <td>4.007890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Liar Liar (1997)</td>\n",
       "      <td>485</td>\n",
       "      <td>3.156701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>English Patient, The (1996)</td>\n",
       "      <td>481</td>\n",
       "      <td>3.656965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Scream (1996)</td>\n",
       "      <td>478</td>\n",
       "      <td>3.441423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>452</td>\n",
       "      <td>3.878319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Air Force One (1997)</td>\n",
       "      <td>431</td>\n",
       "      <td>3.631090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Independence Day (ID4) (1996)</td>\n",
       "      <td>429</td>\n",
       "      <td>3.438228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Raiders of the Lost Ark (1981)</td>\n",
       "      <td>420</td>\n",
       "      <td>4.252381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Godfather, The (1972)</td>\n",
       "      <td>413</td>\n",
       "      <td>4.283293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "      <td>394</td>\n",
       "      <td>4.060914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Twelve Monkeys (1995)</td>\n",
       "      <td>392</td>\n",
       "      <td>3.798469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Silence of the Lambs, The (1991)</td>\n",
       "      <td>390</td>\n",
       "      <td>4.289744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Jerry Maguire (1996)</td>\n",
       "      <td>384</td>\n",
       "      <td>3.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Rock, The (1996)</td>\n",
       "      <td>378</td>\n",
       "      <td>3.693122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Empire Strikes Back, The (1980)</td>\n",
       "      <td>367</td>\n",
       "      <td>4.204360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Star Trek: First Contact (1996)</td>\n",
       "      <td>365</td>\n",
       "      <td>3.660274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Titanic (1997)</td>\n",
       "      <td>350</td>\n",
       "      <td>4.245714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               movie_title  num_ratings  avg_rating\n",
       "movie_id                                                           \n",
       "50                        Star Wars (1977)          583    4.358491\n",
       "258                         Contact (1997)          509    3.803536\n",
       "100                           Fargo (1996)          508    4.155512\n",
       "181              Return of the Jedi (1983)          507    4.007890\n",
       "294                       Liar Liar (1997)          485    3.156701\n",
       "286            English Patient, The (1996)          481    3.656965\n",
       "288                          Scream (1996)          478    3.441423\n",
       "1                         Toy Story (1995)          452    3.878319\n",
       "300                   Air Force One (1997)          431    3.631090\n",
       "121          Independence Day (ID4) (1996)          429    3.438228\n",
       "174         Raiders of the Lost Ark (1981)          420    4.252381\n",
       "127                  Godfather, The (1972)          413    4.283293\n",
       "56                     Pulp Fiction (1994)          394    4.060914\n",
       "7                    Twelve Monkeys (1995)          392    3.798469\n",
       "98        Silence of the Lambs, The (1991)          390    4.289744\n",
       "237                   Jerry Maguire (1996)          384    3.710938\n",
       "117                       Rock, The (1996)          378    3.693122\n",
       "172        Empire Strikes Back, The (1980)          367    4.204360\n",
       "222        Star Trek: First Contact (1996)          365    3.660274\n",
       "313                         Titanic (1997)          350    4.245714"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_stats.sort_values(\"num_ratings\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_train, ml_test = sklearn.model_selection.train_test_split(ml, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to utility matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a mask for some later steps, hence the m > 0 step; ratings go only from 1 to 5, so values of 0 are automatically unknown/missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df2mat(df):\n",
    "    m = np.zeros((max_user, max_movie))\n",
    "    m[df[\"user_id\"], df[\"movie_id\"]] = df[\"rating\"]\n",
    "    return m, m > 0\n",
    "ml_mat_train, ml_mask_train = df2mat(ml_train)\n",
    "ml_mat_test,  ml_mask_test  = df2mat(ml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were an actual large amount of data, which a 944x1683 matrix doesn't really count as, you'd probably want [sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html) and to use 8-bit ints rather than 32-bit floats, for instance:\n",
    "\n",
    "```python\n",
    "ml_mat = scipy.sparse.coo_matrix(\n",
    "    (ml[\"rating\"], (ml[\"user_id\"], ml[\"movie_id\"])),\n",
    "    shape=(max_user, max_movie),\n",
    "    dtype=np.int8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope One implementation\n",
    "\n",
    "- Based on:  [Slope One Predictors for Online Rating-Based Collaborative Filtering](https://arxiv.org/pdf/cs/0702144v1.pdf)\n",
    "- TODO: This needs better explanation but I'm not sure if it should reside here, in the Python code, or in the blag post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deviation(M, mask):\n",
    "    m,n = M.shape\n",
    "    m2 = mask.astype(np.int)\n",
    "    counts = m2.T @ m2\n",
    "    S = m2.T @ M\n",
    "    diffs = S.T - S\n",
    "    dev = diffs / np.maximum(1, counts)\n",
    "    return dev, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of 'deviation' above might be less-optimal vastly larger matrices. For one thing, Slope One doesn't really *need* a utility matrix, though it's easier from one. One could readily compute deviation from the list of ratings, though I don't know a fast way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one(M, mask, dev, counts, u, j, weighted = False):\n",
    "    m,n = M.shape\n",
    "    # S_u is a mask over M's columns for items user 'u' rated:\n",
    "    S_u = mask[u, :]\n",
    "    if weighted:\n",
    "        # In 'Weighted Slope One', we sum over everything user 'u' rated,\n",
    "        # regardless of whether other users rated both this and item j:\n",
    "        S_u[j] = False\n",
    "        c_j = counts[j, S_u]\n",
    "        devs = dev[j, S_u]\n",
    "        u = M[u, S_u]\n",
    "        return ((devs + u) * c_j).sum() / max(1.0, c_j.sum())\n",
    "    else:\n",
    "        # In the 'Slope One' formula we are summing over R_j, which is:\n",
    "        # Every item 'i' (i != j), such that: user 'u' rated item 'i', and\n",
    "        # at least one other user rated both item 'i' and item 'j'.\n",
    "        # Below we compute this likewise as a mask over M's columns:\n",
    "        R_j = S_u * (counts[u, :] > 0)\n",
    "        R_j[j] = False\n",
    "        u = M[u, R_j].sum()\n",
    "        devs = dev[j, R_j].sum()\n",
    "        card = max(1.0, R_j.sum())\n",
    "        return (u + devs) / card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(M, mask, dev, counts, dataframe, weighted=False):\n",
    "    err_mae = 0\n",
    "    err_rms = 0\n",
    "    for row in dataframe.itertuples():\n",
    "        p = predict_one(M, mask, dev, counts,\n",
    "                        row.user_id, row.movie_id, weighted=weighted)\n",
    "        err_mae += np.abs(p - row.rating)\n",
    "        err_rms += np.square(p - row.rating)\n",
    "    err_mae = err_mae / len(dataframe)\n",
    "    err_rms = np.sqrt(err_rms / len(dataframe))\n",
    "    return err_mae, err_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute deviation (which is basically our model) from training:\n",
    "dev, counts = deviation(ml_mat_train, ml_mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.6655537410406179, RMS=0.8512228253964512\n",
      "Testing error: MAE=0.7543173167845862, RMS=0.9626572451178984\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train, True)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test,  True)\n",
    "# why must I pass both dataframe and matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.7383013392450227, RMS=0.9839407186975825\n",
      "Testing error: MAE=0.8959770441268928, RMS=1.2432122955992078\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"SVD\" based algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References on this can be found in a few places.  [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) in Surprise gives enough formulas to implement from. Simon Funk's post [Netflix Update: Try This at Home](http://sifter.org/~simon/journal/20061211.html) is an excellent overview of the rationale and of some practical concerns on how to run this on the much larger Netflix dataset (100,000,000 ratings, instead of 100,000). The paywalled article [Matrix Factorization Techniques for Recommender Systems](http://ieeexplore.ieee.org/abstract/document/5197422/) gives a little background from a higher level.\n",
    "\n",
    "It is also a fairly simple algorithm, but its naming (or lack thereof) is a little confusing. Despite being called an \"SVD\" algorithm, it doesn't exactly compute Singular Value Decomposition, but it's SVD-like. It is still more or less a [matrix completion](https://en.wikipedia.org/wiki/Matrix_completion) problem - but it is a bit different from many matrix completion methods that explicitly use SVD, like [altMinSense & altMinComplete](https://arxiv.org/pdf/1212.0467), which [matrix-completion-whirlwind](https://github.com/asberk/matrix-completion-whirlwind/blob/master/matrix_completion_master.ipynb) has an excellent explanation and implementation of.\n",
    "\n",
    "In short: This algorithm performs matrix completion via low-rank matrix factorization.  It does that factorization via [stochastic gradient-descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), and it does it without explicitly creating a utility matrix.\n",
    "\n",
    "(TODO: Explain utility matrix.)\n",
    "\n",
    "(TODO: Explain what matrices it's even building.)\n",
    "\n",
    "For completeness (and to verify it myself), I give the model and derive its gradients here.\n",
    "\n",
    "The prediction model is:\n",
    "\n",
    "$$\\hat{r}_{ui}=\\mu + b_i + b_u + q_i^\\top p_u$$\n",
    "\n",
    "where $u$ is a user, $i$ is an item, $\\mu$ is the overall average rating, $b_i$ and $b_u$ are per-item and per-user deviations respectively, and $q_i$ and $p_u$ are feature vectors.\n",
    "\n",
    "The goal is to minimize $L_2$-regularized squared error:\n",
    "\n",
    "$$E=\\sum_{r_{ui} \\in R_{\\textrm{train}}} \\left(r_{ui} - \\hat{r}_{ui}\\right)^2 + \\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right)$$\n",
    "\n",
    "This is easily differentiable with respect to model parameters $b_i$, $b_u$, $q_i$, and $p_u$, so a normal approach is gradient-descent.  Finding gradient with respect to $b_i$ is straightforward:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial b_i} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial b_i}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} 2\\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)(-1) + 2 \\lambda b_i \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + r_{ui} - \\hat{r}_{ui}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Gradient with respect to $p_u$ proceeds similarly:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial p_u} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial p_u}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)\\left(-\\frac{\\partial}{\\partial\n",
    "p_u}q_i^\\top p_u \\right) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)(-q_i^\\top) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - \\left(r_{ui} - \\hat{r}_{ui}\\right)q_i^\\top\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Gradient with respect to $b_u$ is identical form to $b_i$, and gradient with respect to $q_i$ is identical form to $p_u$, except that the variables switch places.  The full gradients then have the standard form for gradient descent, i.e. a summation of a gradient term for each individual data point, so they turn easily into update rules for each parameter (which match the ones in the Surprise link) after absorbing the leading 2 into learning rate $\\gamma$ and separating out the summation over each data point. That's given below, with $e_{ui}=r_{ui} - \\hat{r}_{ui}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_i' &= b_i - \\gamma\\frac{\\partial E}{\\partial b_i} &= b_i + \\gamma\\left(e_{ui} - \\lambda b_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_u} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_u + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_u' &= b_u - \\gamma\\frac{\\partial E}{\\partial b_u} &= b_u + \\gamma\\left(e_{ui} - \\lambda b_i \\right)\\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - e_{ui}q_i^\\top\\ \\ \\ &\\longrightarrow p_u' &= p_u - \\gamma\\frac{\\partial E}{\\partial p_u} &= p_u + \\gamma\\left(e_{ui}q_i - \\lambda p_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial q_i} &= 2 \\sum_{r_{ui}} \\lambda q_i - e_{ui}p_u^\\top\\ \\ \\ &\\longrightarrow q_i' &= q_i - \\gamma\\frac{\\partial E}{\\partial q_i} &= q_i + \\gamma\\left(e_{ui}p_u - \\lambda q_i \\right) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Below is a pretty direct implementation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (using Surprise defaults):\n",
    "gamma = 0.005\n",
    "lambda_ = 0.02\n",
    "factors = 100\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average rating:\n",
    "mu = ml[\"rating\"].mean()\n",
    "# TODO: Weight 'mu' by number of ratings?\n",
    "# Deviations per-item and per-user:\n",
    "b_i = np.zeros((max_movie,))\n",
    "b_u = np.zeros((max_user,))\n",
    "# Factor matrices:\n",
    "q_i = np.random.randn(factors, max_movie) * 0.1\n",
    "p_u = np.random.randn(factors, max_user) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svd_predict(items, users, b_i, b_u, q, p):\n",
    "    \"\"\"Returns prediction from the given arrays of items and users,\n",
    "    on this SVD model.\n",
    "    \n",
    "    Parameters:\n",
    "    items -- 1D array of item IDs\n",
    "    users -- 1D array of user IDs (same length as :items:)\n",
    "    b_i -- 1D array of per-item deviations, length N\n",
    "    b_u -- 1D array of per-user deviations, length M\n",
    "    q -- 2D factor matrix as a NumPy array, shape (F,M)\n",
    "    p -- 2D factor matrix as a NumPy array, shape (F,N)\n",
    "    \"\"\"\n",
    "    return mu + b_i[items] + b_u[users] + (q_i[:, items] * p_u[:, users]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svd_error(df, b_i, b_u, q, p):\n",
    "    p = svd_predict(df.movie_id, df.user_id, b_i, b_u, q_i, p_u)\n",
    "    rmse = np.sqrt(np.square(p - df.rating).sum() / len(df))\n",
    "    mae = np.abs(p - df.rating).sum() / len(df)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20; Training: MAE=0.804 RMSE=0.994, Testing: MAE=0.819 RMSE=1.013\n",
      "Epoch 02/20; Training: MAE=0.762 RMSE=0.953, Testing: MAE=0.787 RMSE=0.985\n",
      "Epoch 03/20; Training: MAE=0.741 RMSE=0.930, Testing: MAE=0.773 RMSE=0.973\n",
      "Epoch 04/20; Training: MAE=0.726 RMSE=0.914, Testing: MAE=0.766 RMSE=0.966\n",
      "Epoch 05/20; Training: MAE=0.714 RMSE=0.900, Testing: MAE=0.761 RMSE=0.961\n",
      "Epoch 06/20; Training: MAE=0.704 RMSE=0.887, Testing: MAE=0.758 RMSE=0.958\n",
      "Epoch 07/20; Training: MAE=0.694 RMSE=0.876, Testing: MAE=0.755 RMSE=0.955\n",
      "Epoch 08/20; Training: MAE=0.685 RMSE=0.864, Testing: MAE=0.753 RMSE=0.953\n",
      "Epoch 09/20; Training: MAE=0.676 RMSE=0.853, Testing: MAE=0.751 RMSE=0.951\n",
      "Epoch 10/20; Training: MAE=0.666 RMSE=0.841, Testing: MAE=0.750 RMSE=0.949\n",
      "Epoch 11/20; Training: MAE=0.657 RMSE=0.828, Testing: MAE=0.748 RMSE=0.948\n",
      "Epoch 12/20; Training: MAE=0.646 RMSE=0.815, Testing: MAE=0.747 RMSE=0.947\n",
      "Epoch 13/20; Training: MAE=0.636 RMSE=0.802, Testing: MAE=0.746 RMSE=0.946\n",
      "Epoch 14/20; Training: MAE=0.624 RMSE=0.787, Testing: MAE=0.745 RMSE=0.944\n",
      "Epoch 15/20; Training: MAE=0.613 RMSE=0.772, Testing: MAE=0.744 RMSE=0.943\n",
      "Epoch 16/20; Training: MAE=0.600 RMSE=0.757, Testing: MAE=0.743 RMSE=0.943\n",
      "Epoch 17/20; Training: MAE=0.588 RMSE=0.741, Testing: MAE=0.742 RMSE=0.942\n",
      "Epoch 18/20; Training: MAE=0.575 RMSE=0.724, Testing: MAE=0.741 RMSE=0.941\n",
      "Epoch 19/20; Training: MAE=0.562 RMSE=0.708, Testing: MAE=0.741 RMSE=0.941\n",
      "Epoch 20/20; Training: MAE=0.549 RMSE=0.691, Testing: MAE=0.740 RMSE=0.941\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    idxs = np.random.permutation(len(ml_train))\n",
    "    data = ml_train.iloc[idxs]\n",
    "    # One epoch = SGD on each data point (randomized):\n",
    "    for idx,row in enumerate(ml_train.itertuples()):\n",
    "        u = row.user_id\n",
    "        i = row.movie_id\n",
    "        r_ui = row.rating\n",
    "        e_ui = r_ui - svd_predict(i, u, b_i, b_u, q_i, p_u) # (mu + b_i[i] + b_u[u] + q_i[:, i].T @ p_u[:, u])\n",
    "        dbi = gamma * (e_ui - lambda_ * b_u[u])\n",
    "        dbu = gamma * (e_ui - lambda_ * b_i[i])\n",
    "        dpu = gamma * (e_ui * q_i[:,i] - lambda_ * p_u[:, u])\n",
    "        dqi = gamma * (e_ui * p_u[:,u] - lambda_ * q_i[:, i])\n",
    "        b_i[i] += dbi\n",
    "        b_u[u] += dbu\n",
    "        p_u[:,u] += dpu\n",
    "        q_i[:,i] += dqi\n",
    "    # At each epoch, get training & testing error:\n",
    "    train_rmse, train_mae = svd_error(ml_train, b_i, b_u, q_i, p_u)\n",
    "    test_rmse, test_mae = svd_error(ml_test, b_i, b_u, q_i, p_u)\n",
    "    print(\"Epoch {:02d}/{}; Training: MAE={:.3f} RMSE={:.3f}, Testing: MAE={:.3f} RMSE={:.3f}\".format(epoch + 1, num_epochs, train_mae, train_rmse, test_mae, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations in `scikit-surprise`\n",
    "\n",
    "[Surprise](http://surpriselib.com/) contains implementations of many of the same things, so these are tested below. This same dataset is included as a built-in, but for consistency, we may as well load it from our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would I turn movies into factor scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ml[[\"user_id\", \"movie_id\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.10839986801147461,\n",
       "  0.10835003852844238,\n",
       "  0.1086585521697998,\n",
       "  0.10732698440551758,\n",
       "  0.10420656204223633),\n",
       " 'test_mae': array([ 1.21711932,  1.22271572,  1.22381548,  1.21559752,  1.22134108]),\n",
       " 'test_rmse': array([ 1.51721043,  1.52219415,  1.52415119,  1.51495726,  1.51840742]),\n",
       " 'test_time': (0.23361635208129883,\n",
       "  0.23522114753723145,\n",
       "  0.23500919342041016,\n",
       "  0.23406243324279785,\n",
       "  0.2762880325317383)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.NormalPredictor(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.7369599342346191,\n",
       "  0.7611682415008545,\n",
       "  0.948178768157959,\n",
       "  1.1393687725067139,\n",
       "  0.7258105278015137),\n",
       " 'test_mae': array([ 0.74041847,  0.74460844,  0.74199455,  0.74515014,  0.74241609]),\n",
       " 'test_rmse': array([ 0.94302116,  0.9465838 ,  0.94147874,  0.94827218,  0.94654138]),\n",
       " 'test_time': (3.0613796710968018,\n",
       "  2.5841076374053955,\n",
       "  2.9052228927612305,\n",
       "  3.0102553367614746,\n",
       "  2.3535008430480957)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SlopeOne(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (5.512656211853027,\n",
       "  5.4407196044921875,\n",
       "  5.841166257858276,\n",
       "  5.453616619110107,\n",
       "  4.953552722930908),\n",
       " 'test_mae': array([ 0.74090545,  0.74024858,  0.73754237,  0.73748592,  0.73526663]),\n",
       " 'test_rmse': array([ 0.93790224,  0.93950221,  0.93363063,  0.94017359,  0.93339002]),\n",
       " 'test_time': (0.30484724044799805,\n",
       "  0.26099610328674316,\n",
       "  0.2594015598297119,\n",
       "  0.2566986083984375,\n",
       "  0.2938072681427002)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SVD(), data, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

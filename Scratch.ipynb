{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (Movie Reviews)\n",
    "- Done as part of a SharpestMinds skills test\n",
    "- Author: Chris Hodapp\n",
    "- Date: 2018-02-04\n",
    "\n",
    "This assumes that `ml-100k` from movielens data (should be the `ml-100k` archive from https://grouplens.org/datasets/movielens/100k/) has been downloaded and uncompressed in the local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- Bipolar Slope One?\n",
    "- Read Netflix post\n",
    "- Maybe implement something easy like NMF?\n",
    "- Finish blog post and link to it\n",
    "- Visualization of latent factors\n",
    "- Explain code a bit better (ditch the formal Python version)\n",
    "- Put utility matrix someplace else since I only need it for Slope One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", header=None,\n",
    "                 names=(\"user_id\", \"movie_id\", \"rating\", \"time\"))\n",
    "# Convert Unix seconds to a Pandas timestamp:\n",
    "ml[\"time\"] = pd.to_datetime(ml[\"time\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-01-07 14:20:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-12-03 17:51:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-04-03 18:34:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-02-01 09:20:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-31 21:16:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating                time\n",
       "0      196       242       3 1997-12-04 15:55:49\n",
       "1      186       302       3 1998-04-04 19:22:22\n",
       "2       22       377       1 1997-11-07 07:18:36\n",
       "3      244        51       2 1997-11-27 05:02:03\n",
       "4      166       346       1 1998-02-02 05:33:16\n",
       "5      298       474       4 1998-01-07 14:20:06\n",
       "6      115       265       2 1997-12-03 17:51:28\n",
       "7      253       465       5 1998-04-03 18:34:27\n",
       "8      305       451       3 1998-02-01 09:20:17\n",
       "9        6        86       3 1997-12-31 21:16:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 1683, 1588752)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_user  = int(ml[\"user_id\"].max() + 1)\n",
    "max_movie = int(ml[\"movie_id\"].max() + 1)\n",
    "max_user, max_movie, max_user * max_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of data sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06294248567429025"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.shape[0] / (max_user * max_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_train, ml_test = sklearn.model_selection.train_test_split(ml, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to utility matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a mask for some later steps, hence the m > 0 step; ratings go only from 1 to 5, so values of 0 are automatically unknown/missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df2mat(df):\n",
    "    m = np.zeros((max_user, max_movie))\n",
    "    m[df[\"user_id\"], df[\"movie_id\"]] = df[\"rating\"]\n",
    "    return m, m > 0\n",
    "ml_mat_train, ml_mask_train = df2mat(ml_train)\n",
    "ml_mat_test,  ml_mask_test  = df2mat(ml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were an actual large amount of data, which a 944x1683 matrix doesn't really count as, you'd probably want [sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html) and to use 8-bit ints rather than 32-bit floats, for instance:\n",
    "\n",
    "```python\n",
    "ml_mat = scipy.sparse.coo_matrix(\n",
    "    (ml[\"rating\"], (ml[\"user_id\"], ml[\"movie_id\"])),\n",
    "    shape=(max_user, max_movie),\n",
    "    dtype=np.int8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope One implementation\n",
    "\n",
    "- Based on:  [Slope One Predictors for Online Rating-Based Collaborative Filtering](https://arxiv.org/pdf/cs/0702144v1.pdf)\n",
    "- TODO: This needs better explanation but I'm not sure if it should reside here, in the Python code, or in the blag post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deviation(M, mask):\n",
    "    m,n = M.shape\n",
    "    m2 = mask.astype(np.int)\n",
    "    counts = m2.T @ m2\n",
    "    S = m2.T @ M\n",
    "    diffs = S.T - S\n",
    "    dev = diffs / np.maximum(1, counts)\n",
    "    return dev, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of 'deviation' above might be less-optimal vastly larger matrices. For one thing, Slope One doesn't really *need* a utility matrix, though it's easier from one. One could readily compute deviation from the list of ratings, though I don't know a fast way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one(M, mask, dev, counts, u, j, weighted = False):\n",
    "    m,n = M.shape\n",
    "    # S_u is a mask over M's columns for items user 'u' rated:\n",
    "    S_u = mask[u, :]\n",
    "    if weighted:\n",
    "        # In 'Weighted Slope One', we sum over everything user 'u' rated,\n",
    "        # regardless of whether other users rated both this and item j:\n",
    "        S_u[j] = False\n",
    "        c_j = counts[j, S_u]\n",
    "        devs = dev[j, S_u]\n",
    "        u = M[u, S_u]\n",
    "        return ((devs + u) * c_j).sum() / max(1.0, c_j.sum())\n",
    "    else:\n",
    "        # In the 'Slope One' formula we are summing over R_j, which is:\n",
    "        # Every item 'i' (i != j), such that: user 'u' rated item 'i', and\n",
    "        # at least one other user rated both item 'i' and item 'j'.\n",
    "        # Below we compute this likewise as a mask over M's columns:\n",
    "        R_j = S_u * (counts[u, :] > 0)\n",
    "        R_j[j] = False\n",
    "        u = M[u, R_j].sum()\n",
    "        devs = dev[j, R_j].sum()\n",
    "        card = max(1.0, R_j.sum())\n",
    "        return (u + devs) / card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(M, mask, dev, counts, dataframe, weighted=False):\n",
    "    err_mae = 0\n",
    "    err_rms = 0\n",
    "    for row in dataframe.itertuples():\n",
    "        p = predict_one(M, mask, dev, counts,\n",
    "                        row.user_id, row.movie_id, weighted=weighted)\n",
    "        err_mae += np.abs(p - row.rating)\n",
    "        err_rms += np.square(p - row.rating)\n",
    "    err_mae = err_mae / len(dataframe)\n",
    "    err_rms = np.sqrt(err_rms / len(dataframe))\n",
    "    return err_mae, err_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute deviation (which is basically our model) from training:\n",
    "dev, counts = deviation(ml_mat_train, ml_mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.6655537410406179, RMS=0.8512228253964512\n",
      "Testing error: MAE=0.7543173167845862, RMS=0.9626572451178984\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_mae_train, err_rms_train = predict(ml_mat_train, ml_mask_train, dev, counts, ml_train, True)\n",
    "err_mae_test,  err_rms_test  = predict(ml_mat_test,  ml_mask_test,  dev, counts, ml_test,  True)\n",
    "# why must I pass both dataframe and matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: MAE=0.7383013392450227, RMS=0.9839407186975825\n",
      "Testing error: MAE=0.8959770441268928, RMS=1.2432122955992078\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error: MAE={}, RMS={}\".format(err_mae_train, err_rms_train))\n",
    "print(\"Testing error: MAE={}, RMS={}\".format(err_mae_test, err_rms_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on [Matrix Factorization Techniques for Recommender Systems](http://ieeexplore.ieee.org/abstract/document/5197422/) (sorry, it's paywalled) which shows the same model in equation 5 but not the gradient. [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) in Surprise uses the same equation, and gives the same formulas for gradient-descent.  I derive the gradient below just to verify it myself.\n",
    "\n",
    "The prediction model is:\n",
    "\n",
    "$$\\hat{r}_{ui}=\\mu + b_i + b_u + q_i^\\top p_u$$\n",
    "\n",
    "where $u$ is a user, $i$ is an item, $\\mu$ is the overall average rating, $b_i$ and $b_u$ are per-item and per-user deviations respectively, and $q_i$ and $p_u$ are feature vectors.\n",
    "\n",
    "The goal is to minimize $L_2$-regularized squared error:\n",
    "\n",
    "$$E=\\sum_{r_{ui} \\in R_{\\textrm{train}}} \\left(r_{ui} - \\hat{r}_{ui}\\right)^2 + \\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right)$$\n",
    "\n",
    "This is easily differentiable with respect to model parameters $b_i$, $b_u$, $q_i$, and $p_u$, so a normal approach is gradient-descent.  Finding gradient with respect to $b_i$ is straightforward:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial b_i} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial b_i}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= \\sum_{r_{ui}} 2\\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)(-1) + 2 \\lambda b_i \\\\\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + r_{ui} - \\hat{r}_{ui}\\right)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient with respect to $p_u$ proceeds similarly:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} \\frac{\\partial}{\\partial p_u} \\left(r_{ui} - (\\mu + b_i + b_u + q_i^\\top p_u)\\right)^2 + \\frac{\\partial}{\\partial p_u}\\lambda\\left(b_i^2+b_u^2 + \\lvert\\lvert q_i\\rvert\\rvert^2 + \\lvert\\lvert p_u\\rvert\\rvert^2\\right) \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)\\left(-\\frac{\\partial}{\\partial\n",
    "p_u}q_i^\\top p_u \\right) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= \\sum_{r_{ui}} 2\\left(r_{ui} - \\hat{r}_{ui}\\right)(-q_i^\\top) + 2 \\lambda p_u \\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - \\left(r_{ui} - \\hat{r}_{ui}\\right)q_i^\\top\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Gradient with respect to $b_u$ is identical form to $b_i$, and gradient with respect to $q_i$ is identical form to $p_u$, except that the variables switch places.  The full gradients then have the standard form for gradient descent, i.e. a summation of a gradient term for each individual data point, so they turn easily into update rules for each parameter (which match the ones in the Surprise link) after absorbing the leading 2 into learning rate $\\gamma$ and separating out the summation over each data point. That's given below, with $e_{ui}=r_{ui} - \\hat{r}_{ui}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial b_i} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_i + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_i' &= b_i - \\gamma\\frac{\\partial E}{\\partial b_i} &= b_i + \\gamma\\left(e_{ui} - \\lambda b_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial b_u} &= 2 \\sum_{r_{ui}} \\left(\\lambda b_u + e_{ui}\\right)\\ \\ \\ &\\longrightarrow b_u' &= b_u - \\gamma\\frac{\\partial E}{\\partial b_u} &= b_u + \\gamma\\left(e_{ui} - \\lambda b_i \\right)\\\\\n",
    "\\frac{\\partial E}{\\partial p_u} &= 2 \\sum_{r_{ui}} \\lambda p_u - e_{ui}q_i^\\top\\ \\ \\ &\\longrightarrow p_u' &= p_u - \\gamma\\frac{\\partial E}{\\partial p_u} &= p_u + \\gamma\\left(e_{ui}q_i - \\lambda p_u \\right) \\\\\n",
    "\\frac{\\partial E}{\\partial q_i} &= 2 \\sum_{r_{ui}} \\lambda q_i - e_{ui}p_u^\\top\\ \\ \\ &\\longrightarrow q_i' &= q_i - \\gamma\\frac{\\partial E}{\\partial q_i} &= q_i + \\gamma\\left(e_{ui}p_u - \\lambda q_i \\right) \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (using Surprise defaults):\n",
    "gamma = 0.005\n",
    "lambda_ = 0.02\n",
    "factors = 100\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average rating:\n",
    "mu = ml[\"rating\"].mean()\n",
    "# Deviations per-item and per-user (initialized randomly):\n",
    "b_i = np.zeros((max_movie,))\n",
    "b_u = np.zeros((max_user,))\n",
    "# Factor matrices:\n",
    "q_i = np.random.randn(factors, max_movie) * 0.1\n",
    "p_u = np.random.randn(factors, max_user) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svd_predict(items, users, b_i, b_u, q, p):\n",
    "    \"\"\"Returns prediction from the given arrays of items and users,\n",
    "    on this SVD model.\n",
    "    \n",
    "    Parameters:\n",
    "    items -- 1D array of item IDs\n",
    "    users -- 1D array of user IDs (same length as :items:)\n",
    "    b_i -- 1D array of per-item deviations, length N\n",
    "    b_u -- 1D array of per-user deviations, length M\n",
    "    q -- 2D factor matrix as a NumPy array, shape (F,M)\n",
    "    p -- 2D factor matrix as a NumPy array, shape (F,N)\n",
    "    \"\"\"\n",
    "    return mu + b_i[items] + b_u[users] + (q_i[:, items] * p_u[:, users]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svd_error(df, b_i, b_u, q, p):\n",
    "    p = svd_predict(df.movie_id, df.user_id, b_i, b_u, q_i, p_u)\n",
    "    rmse = np.sqrt(np.square(p - df.rating).sum() / len(df))\n",
    "    mae = np.abs(p - df.rating).sum() / len(df)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20; Training: MAE=0.804 RMSE=0.994, Testing: MAE=0.818 RMSE=1.012\n",
      "Epoch 02/20; Training: MAE=0.762 RMSE=0.954, Testing: MAE=0.787 RMSE=0.985\n",
      "Epoch 03/20; Training: MAE=0.741 RMSE=0.930, Testing: MAE=0.774 RMSE=0.973\n",
      "Epoch 04/20; Training: MAE=0.726 RMSE=0.914, Testing: MAE=0.766 RMSE=0.966\n",
      "Epoch 05/20; Training: MAE=0.714 RMSE=0.900, Testing: MAE=0.761 RMSE=0.961\n",
      "Epoch 06/20; Training: MAE=0.704 RMSE=0.887, Testing: MAE=0.758 RMSE=0.958\n",
      "Epoch 07/20; Training: MAE=0.694 RMSE=0.875, Testing: MAE=0.755 RMSE=0.955\n",
      "Epoch 08/20; Training: MAE=0.685 RMSE=0.864, Testing: MAE=0.753 RMSE=0.953\n",
      "Epoch 09/20; Training: MAE=0.676 RMSE=0.852, Testing: MAE=0.752 RMSE=0.952\n",
      "Epoch 10/20; Training: MAE=0.666 RMSE=0.841, Testing: MAE=0.750 RMSE=0.950\n",
      "Epoch 11/20; Training: MAE=0.657 RMSE=0.828, Testing: MAE=0.749 RMSE=0.949\n",
      "Epoch 12/20; Training: MAE=0.646 RMSE=0.815, Testing: MAE=0.748 RMSE=0.948\n",
      "Epoch 13/20; Training: MAE=0.636 RMSE=0.802, Testing: MAE=0.747 RMSE=0.947\n",
      "Epoch 14/20; Training: MAE=0.624 RMSE=0.787, Testing: MAE=0.746 RMSE=0.946\n",
      "Epoch 15/20; Training: MAE=0.613 RMSE=0.772, Testing: MAE=0.745 RMSE=0.945\n",
      "Epoch 16/20; Training: MAE=0.600 RMSE=0.757, Testing: MAE=0.744 RMSE=0.944\n",
      "Epoch 17/20; Training: MAE=0.588 RMSE=0.741, Testing: MAE=0.743 RMSE=0.943\n",
      "Epoch 18/20; Training: MAE=0.575 RMSE=0.724, Testing: MAE=0.742 RMSE=0.943\n",
      "Epoch 19/20; Training: MAE=0.562 RMSE=0.708, Testing: MAE=0.742 RMSE=0.942\n",
      "Epoch 20/20; Training: MAE=0.549 RMSE=0.691, Testing: MAE=0.741 RMSE=0.942\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    idxs = np.random.permutation(len(ml_train))\n",
    "    data = ml_train.iloc[idxs]\n",
    "    for idx,row in enumerate(ml_train.itertuples()):\n",
    "        u = row.user_id\n",
    "        i = row.movie_id\n",
    "        r_ui = row.rating\n",
    "        e_ui = r_ui - svd_predict(i, u, b_i, b_u, q_i, p_u) # (mu + b_i[i] + b_u[u] + q_i[:, i].T @ p_u[:, u])\n",
    "        dbi = gamma * (e_ui - lambda_ * b_u[u])\n",
    "        dbu = gamma * (e_ui - lambda_ * b_i[i])\n",
    "        dpu = gamma * (e_ui * q_i[:,i] - lambda_ * p_u[:, u])\n",
    "        dqi = gamma * (e_ui * p_u[:,u] - lambda_ * q_i[:, i])\n",
    "        b_i[i] += dbi\n",
    "        b_u[u] += dbu\n",
    "        p_u[:,u] += dpu\n",
    "        q_i[:,i] += dqi\n",
    "    train_rmse, train_mae = svd_error(ml_train, b_i, b_u, q_i, p_u)\n",
    "    test_rmse, test_mae = svd_error(ml_test, b_i, b_u, q_i, p_u)\n",
    "    print(\"Epoch {:02d}/{}; Training: MAE={:.3f} RMSE={:.3f}, Testing: MAE={:.3f} RMSE={:.3f}\".format(epoch + 1, num_epochs, train_mae, train_rmse, test_mae, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations in `scikit-surprise`\n",
    "\n",
    "[Surprise](http://surpriselib.com/) contains implementations of many of the same things, so these are tested below. This same dataset is included as a built-in, but for consistency, we may as well load it from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ml[[\"user_id\", \"movie_id\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.10839986801147461,\n",
       "  0.10835003852844238,\n",
       "  0.1086585521697998,\n",
       "  0.10732698440551758,\n",
       "  0.10420656204223633),\n",
       " 'test_mae': array([ 1.21711932,  1.22271572,  1.22381548,  1.21559752,  1.22134108]),\n",
       " 'test_rmse': array([ 1.51721043,  1.52219415,  1.52415119,  1.51495726,  1.51840742]),\n",
       " 'test_time': (0.23361635208129883,\n",
       "  0.23522114753723145,\n",
       "  0.23500919342041016,\n",
       "  0.23406243324279785,\n",
       "  0.2762880325317383)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.NormalPredictor(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.7369599342346191,\n",
       "  0.7611682415008545,\n",
       "  0.948178768157959,\n",
       "  1.1393687725067139,\n",
       "  0.7258105278015137),\n",
       " 'test_mae': array([ 0.74041847,  0.74460844,  0.74199455,  0.74515014,  0.74241609]),\n",
       " 'test_rmse': array([ 0.94302116,  0.9465838 ,  0.94147874,  0.94827218,  0.94654138]),\n",
       " 'test_time': (3.0613796710968018,\n",
       "  2.5841076374053955,\n",
       "  2.9052228927612305,\n",
       "  3.0102553367614746,\n",
       "  2.3535008430480957)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SlopeOne(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (5.512656211853027,\n",
       "  5.4407196044921875,\n",
       "  5.841166257858276,\n",
       "  5.453616619110107,\n",
       "  4.953552722930908),\n",
       " 'test_mae': array([ 0.74090545,  0.74024858,  0.73754237,  0.73748592,  0.73526663]),\n",
       " 'test_rmse': array([ 0.93790224,  0.93950221,  0.93363063,  0.94017359,  0.93339002]),\n",
       " 'test_time': (0.30484724044799805,\n",
       "  0.26099610328674316,\n",
       "  0.2594015598297119,\n",
       "  0.2566986083984375,\n",
       "  0.2938072681427002)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise.model_selection.cross_validate(surprise.SVD(), data, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
